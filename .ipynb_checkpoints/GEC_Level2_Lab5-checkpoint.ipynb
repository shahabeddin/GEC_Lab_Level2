{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEC Data Science Program\n",
    "## Level 2: Lab 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data: Airline Monthly Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://datamarket.com/data/set/22u3/international-airline-passengers-monthly-totals-in-thousands-jan-49-dec-60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, ConvLSTM2D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#import pandas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_fit_predict(model, data, predictors, target, split_test_size=0.3):\n",
    "    \"\"\"\n",
    "    1. Split 'data[predictors]','data[target] into train and test using 'split_test_size' ratio\n",
    "    2. Fit 'model' using training data\n",
    "    3. Predict against test data\n",
    "    \"\"\"\n",
    "    if split_test_size == 0:\n",
    "        df_train = data\n",
    "        df_test = None\n",
    "    else:\n",
    "        df_train, df_test = train_test_split(data, test_size=split_test_size)\n",
    "    X_train=df_train[predictors]\n",
    "    X_test=df_test[predictors] if df_test else None\n",
    "    y_train=df_train[target]\n",
    "    y_test=df_test[target] if df_test else None\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred=model.predict(X_test) if df_test else None\n",
    "    y_fit=model.predict(X_train)\n",
    "    return y_pred, y_test, y_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/shahab/Downloads/international-airline-passengers.csv', \n",
    "                          index_col=0, skipfooter=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.columns=[\"passengers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.index=dataset.index.to_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset_n = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: If we have a time series data, how can we predict the future values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have:\n",
    "$t=[1, 2, 3, 4, 5, 6, 7, 8, 9]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform $t$ like this: \n",
    "\n",
    "For each value of $t$ we look at $n$ previous values (look-back), and create a feature matrix.\n",
    "\n",
    "With look-back = 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[1,2,3]->4$\n",
    "\n",
    "$[2,3,4]->5$\n",
    "\n",
    "$[3,4,5]->6$\n",
    "\n",
    "$[4,5,6]->7$\n",
    "\n",
    "$[5,6,7]->8$\n",
    "\n",
    "$[6,7,8]->9$\n",
    "\n",
    "$X=\n",
    "\\left(\\begin{array}{cc} \n",
    "1 & 2 & 3\\\\\n",
    "2 & 3 & 4\\\\\n",
    "3 & 4 & 5\\\\\n",
    "4 & 5 & 6\\\\\n",
    "5 & 6 & 7\\\\\n",
    "6 & 7 & 8\\\\\n",
    "\\end{array}\\right)\n",
    "$\n",
    "$\n",
    "y=\\left(\\begin{array}{cc}\n",
    "4\\\\\n",
    "5\\\\\n",
    "6\\\\\n",
    "7\\\\\n",
    "8\\\\\n",
    "9\\\\\n",
    "\\end{array}\\right)\n",
    "$\n",
    "\n",
    "Now we can train a model using feature matrix $X$ and labels $y$. Then to predict the next value we use input vector $x=[7,8,9]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a look-back dataset matrix\n",
    "def timeseries_to_matrix(timeseries, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(timeseries)-look_back):\n",
    "        a = timeseries[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(timeseries[i + look_back])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=range(1,10)\n",
    "timeseries_to_matrix(t,look_back=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "look_back=5\n",
    "X, y = timeseries_to_matrix(dataset_n[:,0], look_back)\n",
    "#testX_r, testY_r = timeseries_to_matrix(test, look_back=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: How should we split time-series data into train and test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.9)\n",
    "test_size = len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:]\n",
    "\n",
    "y_train = y[:train_size]\n",
    "y_test = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also create arrays for date-time values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_train = dataset.index[look_back:train_size+look_back]\n",
    "T_test = dataset.index[train_size+look_back:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_train.shape, T_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Let's train a simple model on this transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf.fit(X_train,y_train)\n",
    "y_pred_rf=rf.predict(X_test)\n",
    "y_fit_rf=rf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train,y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "y_fit_lr = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_train, y_fit_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_train, y_fit_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # make predictions\n",
    "# trainPredict = model.predict(trainX)\n",
    "# testPredict = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(y_fit_rf)\n",
    "#trainY = scaler.inverse_transform([y_train])\n",
    "testPredict = scaler.inverse_transform(y_pred_rf)\n",
    "#testY = scaler.inverse_transform([y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset,':')\n",
    "plt.plot(T_train,trainPredict)\n",
    "plt.plot(T_test, testPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert predictions\n",
    "trainPredict_lr = scaler.inverse_transform(y_fit_lr)\n",
    "testPredict_lr = scaler.inverse_transform(y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset,':')\n",
    "plt.plot(T_train,trainPredict_lr)\n",
    "plt.plot(T_test, testPredict_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks \n",
    "### Long-Term Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "https://keras.io/layers/recurrent/\n",
    "\n",
    "http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Long Short-Term Memory network (LSTM) is a type of Recurrent Neural Network (RNN).\n",
    "\n",
    "A benefit of this type of network is that it can learn and remember over long sequences and does not rely on a pre-specified window lagged observation as input.\n",
    "\n",
    "The LSTM layer expects input to be in a matrix with the dimensions: [samples, time steps, features].\n",
    "\n",
    "- Samples: These are independent observations from the domain, typically rows of data.\n",
    "\n",
    "- Time steps: These are separate time steps of a given variable for a given observation.\n",
    "\n",
    "- Features: These are separate measures observed at the time of observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy training data\n",
    "x_train_toy = np.random.random((1000, timesteps, data_dim))\n",
    "y_train_toy = np.random.random((1000, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, #return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_toy,y_train_toy, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM for flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # reshape input to be [samples, time steps, features]\n",
    "X_train_r = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_r = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(5, input_shape=(None,look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_r, y_train, nb_epoch=50, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Is it over-fitted or under-fitted? How do we find out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_fit_lstm=model.predict(X_train_r)\n",
    "y_pred_lstm=model.predict(X_test_r)\n",
    "\n",
    "trainPredict_lstm = scaler.inverse_transform(y_fit_lstm)\n",
    "testPredict_lstm = scaler.inverse_transform(y_pred_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict_lstm.shape, testPredict_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset,':')\n",
    "plt.plot(T_train,trainPredict_lstm)\n",
    "plt.plot(T_test, testPredict_lstm,'.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_train,y_fit_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_test,y_pred_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking multiple LSTM Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should use return_sequences=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, #input_dim=look_back))\n",
    "               input_shape=(None, look_back)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',\n",
    "             metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fitting=model.fit(X_train_r, y_train, nb_epoch=50, batch_size=5, verbose=1, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fitting.history['loss'],'b')\n",
    "plt.plot(fitting.history['val_loss'],'g')\n",
    "plt.legend(('training_loss', 'testing_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_fit_lstm=model.predict(X_train_r)\n",
    "y_pred_lstm=model.predict(X_test_r)\n",
    "\n",
    "trainPredict_lstm = scaler.inverse_transform(y_fit_lstm)\n",
    "testPredict_lstm = scaler.inverse_transform(y_pred_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset,':')\n",
    "plt.plot(T_train,trainPredict_lstm)\n",
    "plt.plot(T_test, testPredict_lstm,'.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_train,y_fit_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_test,y_pred_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statefull LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to process longer sequences while keeping computational complexity manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Fix the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True,\n",
    "               input_shape=(None, look_back)))  \n",
    "               # batch_input_shape=(batch_size, None, look_back)))\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True))  \n",
    "model.add(LSTM(32, stateful=True))  \n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',\n",
    "             metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Fix the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting=model.fit(X_train_r, y_train, \n",
    "                  nb_epoch=100, batch_size=batch_size, verbose=1, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fitting.history['loss'],'b')\n",
    "plt.plot(fitting.history['val_loss'],'g')\n",
    "plt.legend(('training_loss', 'testing_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit_lstm=model.predict(X_train_r[?:], batch_size=batch_size)\n",
    "y_pred_lstm=model.predict(X_test_r)\n",
    "\n",
    "trainPredict_lstm = scaler.inverse_transform(y_fit_lstm)\n",
    "testPredict_lstm = scaler.inverse_transform(y_pred_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainPredict_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset,':')\n",
    "plt.plot(T_train[9:],trainPredict_lstm)\n",
    "plt.plot(T_test, testPredict_lstm,'.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_train[?:],y_fit_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_test,y_pred_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is unerfitted. Let's add more epochs and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(60, return_sequences=True, stateful=True,\n",
    "               #input_shape=(None, look_back)))  # returns a sequence of vectors of dimension 32\n",
    "                batch_input_shape=(batch_size, None, look_back)))\n",
    "model.add(LSTM(30, return_sequences=True, stateful=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(20, stateful=True))  # return a single vector of dimension 32\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',\n",
    "             metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=50, verbose=1, mode='auto')\n",
    "fitting=model.fit(X_train_r[-12*batch_size:], y_train[-12*batch_size:], callbacks=[early_stopping],\n",
    "                  nb_epoch=1000, batch_size=batch_size, verbose=1, validation_split=0.25,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fitting.history['loss'],'b')\n",
    "plt.plot(fitting.history['val_loss'],'g')\n",
    "plt.legend(('training_loss', 'testing_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_fit_lstm=model.predict(X_train_r[-12*batch_size:], batch_size=batch_size)\n",
    "y_pred_lstm=model.predict(X_test_r)\n",
    "\n",
    "trainPredict_lstm = scaler.inverse_transform(y_fit_lstm)\n",
    "testPredict_lstm = scaler.inverse_transform(y_pred_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_train[-12*batch_size:],y_fit_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_test,y_pred_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Time Series to Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return np.array(diff, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse_difference(history, yhat, interval=1):\n",
    "    return yhat + history[-interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "differenced = difference(dataset.values, 1)\n",
    "print(differenced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse transform is easy using the first value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0=dataset.values[0]\n",
    "inverted=[t0]\n",
    "for d in differenced:\n",
    "    inverted.append(inverted[-1]+d)\n",
    "inverted=np.array(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(inverted==dataset.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse_difference(t0, differenced):\n",
    "    inverted=[t0]\n",
    "    for d in differenced:\n",
    "        inverted.append(inverted[-1]+d)\n",
    "    inverted=np.array(inverted)\n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differenced_n = scaler.fit_transform(differenced)\n",
    "look_back=5\n",
    "X, y = timeseries_to_matrix(dataset_n[:,0], look_back)\n",
    "dX, dy = timeseries_to_matrix(differenced_n[:,0], look_back)\n",
    "\n",
    "dX_train = dX[:train_size]\n",
    "dX_test = dX[train_size:]\n",
    "\n",
    "dy_train = dy[:train_size]\n",
    "dy_test = dy[train_size:]\n",
    "\n",
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:]\n",
    "\n",
    "y_train = y[:train_size]\n",
    "y_test = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX.shape, dX_train.shape, dX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_train_r = np.reshape(dX_train, (dX_train.shape[0], 1, dX_train.shape[1]))\n",
    "dX_test_r = np.reshape(dX_test, (dX_test.shape[0], 1, dX_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(20, input_shape=(None,look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fitting=model.fit(dX_train_r, dy_train, nb_epoch=200, batch_size=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fitting.history['loss'],'b')\n",
    "plt.plot(fitting.history['val_loss'],'g')\n",
    "plt.legend(('training_loss', 'testing_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dy_fit_lstm=model.predict(dX_train_r)\n",
    "dy_pred_lstm=model.predict(dX_test_r)\n",
    "\n",
    "dtrainPredict_lstm = scaler.inverse_transform(dy_fit_lstm)\n",
    "dtestPredict_lstm = scaler.inverse_transform(dy_pred_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(differenced[look_back:],':')\n",
    "plt.plot(dtrainPredict_lstm)\n",
    "plt.plot(range(len(dtrainPredict_lstm),len(differenced)-look_back),dtestPredict_lstm,'.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(dy_train,dy_fit_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(dy_test,dy_pred_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Video classification methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Classifying one frame at a time with a ConvNet\n",
    "2. Using a time-distributed ConvNet and passing the features to an RNN(LSTM), in one network\n",
    "3. Using a 3D convolutional network\n",
    "4. Extracting features from each frame with a ConvNet and passing the sequence to a separate LSTM\n",
    "5. Extracting features from each frame with a ConvNet and passing the sequence to a separate MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.coast.ai/five-video-classification-methods-implemented-in-keras-and-tensorflow-99cad29cc0b5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/harvitronix/five-video-classification-methods/blob/master/models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best method is #4: CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
